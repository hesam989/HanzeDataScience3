{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 01\n",
    "\n",
    "Study the Tutorial tutorial_cluster_scanpy_object and the tutorial_Clustering_Methods\n",
    "\n",
    "Write a brief summary about the following:\n",
    "\n",
    "1. What are common preprocessing steps? Explain for each step why and when you should execute this step and when not.\n",
    "2.\tWhat visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. \n",
    "\n",
    "Bonus points: do this as well for the scanpy tutorial.\n",
    "3. \tWhat performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation.\n",
    "\n",
    "\n",
    "Bonus:\n",
    "You practice the steps yourself with the breast_cancer dataset (clustering_data.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Common Preprocessing Steps:\n",
    "**Data Cleaning:** This involves handling missing values and outliers in a dataset. This is crucial as they can skew results and hinder the ability of the model to learn effectively. However, if  the data is already clean, we might not need to do this step.\n",
    "\n",
    "**Feature Scaling:** This includes standardization (bringing all features to a standard scale) and normalization (scaling features between 0 and 1). This is essential for most machine learning algorithms as they are sensitive to the range of the data. If the algorithm doesn't assume any particular scale or distribution of data (like decision trees), we might skip this step.\n",
    "\n",
    "**Encoding Categorical Variables:** Many machine learning algorithms require numerical input. If the data includes categorical variables, we'll need to encode them to numerical values. If the data is already numerical, this step is unnecessary.\n",
    "\n",
    "**Dimensionality Reduction:** For datasets with many features, it might be useful to reduce the number of dimensions to simplify the model and avoid overfitting. Techniques like PCA or t-SNE are commonly used. However, if we have a small number of features or when every feature is important, dimensionality reduction might not be necessary.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Visualization Methods in Cluster Methods Tutorial:\n",
    "\n",
    "The common methods for visualizing clustering results are **scatter plots, dendrograms, and heatmaps.**\n",
    "\n",
    "**Scatter plots:** Particularly useful when the data has been reduced to two or three dimensions using techniques like PCA or t-SNE. The resulting clusters can be visualized as different colors or shapes on the plot.\n",
    "\n",
    "**Dendrograms:** These are used often in hierarchical clustering to illustrate the arrangement of the clusters produced by the algorithm.\n",
    "\n",
    "**Heatmaps:** Commonly used alongside hierarchical clustering. They are often used when we have relatively small data and can show the level of similarity between different data points.\n",
    "\n",
    "### 3. Performance/Evaluation Metrics in Cluster Methods Tutorial:\n",
    "\n",
    "The common clustering evaluation metrics are:\n",
    "\n",
    "**Silhouette Score:** Measures how close each sample in one cluster is to the samples in the neighboring clusters. It's a great tool for assessing the quality of our clusters when true labels are not known. A silhouette score of 1 indicates the highest separation between clusters, -1 indicates that data points are in the wrong cluster, and values near 0 indicate overlapping clusters.\n",
    "\n",
    "**Davies-Bouldin Index:** This index signifies the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score and implies a better partition.\n",
    "Rand Index: This measures the similarity between two data clusterings. A Rand Index of 1 indicates that the two data clusterings are identical.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print('Hello world')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
